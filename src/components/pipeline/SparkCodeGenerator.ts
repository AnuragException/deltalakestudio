
import { DataSourceFormValues, CloudStorageProvider } from './types';

export const generateSparkCode = (values: DataSourceFormValues): string => {
  let code = 'df = spark.read';
  
  switch(values.fileFormat) {
    case 'csv':
      code += `\n    .format("csv")`;
      if (values.hasHeader) {
        code += `\n    .option("header", "true")`;
      }
      if (values.inferSchema) {
        code += `\n    .option("inferSchema", "true")`;
      }
      break;
    case 'json':
      code += `\n    .format("json")`;
      if (values.multiLine) {
        code += `\n    .option("multiLine", "true")`;
      }
      break;
    case 'parquet':
      code += `\n    .format("parquet")`;
      break;
    case 'orc':
      code += `\n    .format("orc")`;
      break;
  }
  
  code += `\n    .load("${values.filePath}")`;

  // Add comment for cloud storage provider
  if (values.cloudProvider && values.cloudProvider !== 'local') {
    code += `\n\n# Using ${getCloudProviderFullName(values.cloudProvider)} for data source`;
  }
  
  return code;
};

// Generate a simplified summary of the code for display
export const generateCodeSummary = (values: DataSourceFormValues): string => {
  const provider = values.cloudProvider !== 'local' ? 
    ` from ${getCloudProviderShortName(values.cloudProvider)}` : '';
  return `df = spark.read.format("${values.fileFormat}")...${provider}`;
};

// Helper function to get full provider name
const getCloudProviderFullName = (provider: CloudStorageProvider): string => {
  switch(provider) {
    case 's3': return 'Amazon S3';
    case 'adls': return 'Azure Data Lake Storage Gen2';
    case 'gcs': return 'Google Cloud Storage';
    case 'azure-blob': return 'Azure Blob Storage (Mounted)';
    case 'aws': return 'Amazon Web Services';
    case 'azure': return 'Microsoft Azure';
    case 'gcp': return 'Google Cloud Platform';
    default: return 'Local File System';
  }
};

// Helper function to get short provider name
const getCloudProviderShortName = (provider: CloudStorageProvider): string => {
  switch(provider) {
    case 's3': return 'S3';
    case 'adls': return 'ADLS';
    case 'gcs': return 'GCS';
    case 'azure-blob': return 'Azure Blob';
    case 'aws': return 'AWS';
    case 'azure': return 'Azure';
    case 'gcp': return 'GCP';
    default: return '';
  }
};

// Generate Databricks notebook content
export const generateNotebookCode = (values: DataSourceFormValues): string => {
  let notebookContent = `# Databricks Notebook Generated by Pipeline Designer\n\n`;
  notebookContent += `# Data Source Configuration\n`;
  notebookContent += `# Format: ${values.fileFormat.toUpperCase()}\n`;
  notebookContent += `# Path: ${values.filePath}\n`;
  
  if (values.cloudProvider !== 'local') {
    notebookContent += `# Cloud Provider: ${getCloudProviderFullName(values.cloudProvider)}\n`;
    
    // Add specific cloud configuration code snippets if needed
    if (values.cloudProvider === 'azure-blob') {
      notebookContent += `\n# For Azure Blob Storage, you might need to mount first:\n`;
      notebookContent += `# dbutils.fs.mount(\n`;
      notebookContent += `#   source = "wasbs://<container>@<storage-account>.blob.core.windows.net",\n`;
      notebookContent += `#   mount_point = "/mnt/<mount-name>"\n`;
      notebookContent += `# )\n`;
    }
  }
  
  notebookContent += `\n# PySpark Code\n`;
  notebookContent += generateSparkCode(values);
  
  notebookContent += `\n\n# Display the dataframe\ndisplay(df)`;
  
  return notebookContent;
};
